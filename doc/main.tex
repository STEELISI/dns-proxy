\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{indentfirst}
\usepackage{graphicx}

\title{Answering Invalid TLD Requests at Line Rate with DPDK}
\author{Haoda Wang}
\date{May 2020}

\begin{document}

\maketitle

\section{Program Design}
The program was based on DPDK's KNI example application, which mirrors traffic between a virtual KNI interface and a physical one assigned to DPDK. This also provides a modest increase in performance under normal usage by eliminating some expensive system calls. This comes at the cost of dedicating some cores purely to DPDK. \\

As the KNI interface exists as a standard Ethernet device, the application can be transparent to both the client and the server. Any operations on the interface will have the same result as if it was done to the physical interface. This is useful as all queries containing valid top-level domains can be processed normally. \\

Unlike DPDK's KNI example, this application requires four threads per interface to run at maximum efficiency. In addition, it uses two additional ring buffers to minimize packet loss. One of the threads moves all incoming packets into the \texttt{ETH\_RX\_RING} buffer. \\

Another thread then reads the packets in the \texttt{WORKER\_RX\_RING} buffer, where the following properties of the packet is determined:

\begin{enumerate}
  \item Is this host the destination for the packet?
  \item Is this packet a UDP packet going to port 53?
  \item Is this packet a standard DNS query?
  \item Does this packet contain an invalid TLD?
\end{enumerate}

If the answer to one of the above questions is "no", the packet is immediately moved into the \texttt{KNI\_RX\_RING}, at which point it is processed by the host. However, if the answers to all the questions is "yes", then the packet may be part of an invalid TLD attack, and is moved to the \texttt{WORKER\_TX\_RING}. \\

The third thread takes the packets in \texttt{WORKER\_TX\_RING} and modifies them into NXDOMAIN replies. This is done by swapping the destination and source addresses and ports in the Ethernet, IPv4, and UDP headers. Then, the DNS headers are changed so that the request becomes an authoritative answer. Then the IP checksum is calculated and set, and the packet is put into the \texttt{ETH\_TX\_RING}, where the NIC will send it out. \\

The final thread takes all outgoing traffic from the \texttt{KNI\_TX\_RING} buffer and moves them to the \texttt{ETH\_TX\_RING}. A visualization of the program's architecture can be shown below: \\

\noindent
\includegraphics[width=\textwidth]{buf_layout.png}

\section{Test Setup}
The application was tested using a pair of d430 nodes on Emulab, which contains an Intel X710 10GbE network card. All nodes ran Ubuntu 18.04. Each node had tuned-adm running with the network-throughput profile loaded. \\

One of the nodes was designated as a root node, and had the Bind9 DNS server installed and configured according to the configuration generator at \texttt{localroot.isi.edu}. However, to avoid interfering with legitimate DNS servers, all external master nodes were removed. \\

The \texttt{testing/generate\_bind\_config.py} script in the Git repository was used to generate 200,000 random DNS entries with a randomly selected TLD from ICANN's list of valid TLDs, and a randomly generated UUID as a domain name. The generation script also creates an input file for dnsperf with every valid TLD generated by the script. \\

Tests were done using both dnsperf and DPDK-pktgen. In both cases, 600,000 requests were sent, either using the valid TLDs generated from the bind9 config or invalid, randomly generated UUIDs. As each request is a single packet, the DPDK-pktgen tests assumed that each packet was a request. \\

\section{Results}

The following results are from a dnsperf run using invalid TLDs. The run simulated 1000 DNS clients and ran on 12 threads, with a timeout of 5 seconds. Each client sends requests and waits for a reply before sending the next one. The timeout applies to dropped packets as well, so each dropped packet incurs a penalty of 5 seconds on the DNS client that requested it. \\

\begin{center}
\begin{tabular}{ ||c|c|c|| }
\hline
Attack Traffic & With dns-proxy & Without dns-proxy \\
\hline
Queries sent & 600,000 & 600,000 \\
Queries completed & 597,829 & 600,000 \\
Queries lost & 2,171 & 0 \\
Run time (s) & 107.773797 & 182.762914 \\
Queries per second & 5,547.071892 & 3,282.941746 \\
Average Latency (s) & 0.000097 & 0.030340 \\
Minimum Latency (s) & 0.000017 & 0.002217 \\
Maximum Latency (s) & 0.000657 & 1.122642 \\
Latency StdDev (s) & 0.000031 & 0.030501 \\
\hline
\end{tabular}
\end{center}

The results below show the performance against dnsperf using legitimate traffic. The configuration is identical to the one above, with only the input file changed. \\

\begin{center}
\begin{tabular}{ ||c|c|c|| }
\hline
Legitimate Traffic & With dns-proxy & Without dns-proxy \\
\hline
Queries sent & 600,000 & 600,000 \\
Queries completed & 599,750 & 599,850 \\
Queries lost & 250 & 150 \\
Run time (s) & 35.888684 & 23.945254 \\
Queries per second & 16,711.39572 & 25,050.893175 \\
Average Latency (s) & 0.00373 & 0.00254 \\
Minimum Latency (s) & 0.000117 & 0.000112 \\
Maximum Latency (s) & 0.039361 & 0.334216 \\
Latency StdDev (s) & 0.003353 & 0.010643 \\
\hline
\end{tabular}
\end{center}

To examine the performance of the program under heavy load, DPDK-pktgen was able to send all 600,000 packets at 25\% of line rate using this NIC and record all 600,000 packets returned. Given the line rate of 37Mpps, this application can handle around 9.25Mpps in this configuration. The following graph shows the performance of the application as the packet rate increases. \\

\noindent
\includegraphics[width=\textwidth]{performance.png}

\section{Analysis}
The runtime on dnsperf is not a good measure of actual run time if there is packet loss, since the penalty for a single lost packet is 5 seconds for the client. An easy way to ignore packet loss is to calculate for requests per second using the average latency instead, though this method does not account for the multiple clients that dnsperf simulates:

\begin{center}
\begin{tabular}{ ||c|c|c|| }
\hline
Configuration &  & Requests per second \\
\hline
Attack traffic with dns-proxy & 1/0.000097 & 10309.3 requests/s \\
Attack traffic without dns-proxy & 1/0.030340 & 32.9598 requests/s \\
Legitimate traffic with dns-proxy & 1/0.00373 & 268.097 requests/s \\
Legitimate traffic without dns-proxy & 1/0.00254 & 393.701 requests/s \\
\hline
\end{tabular}
\end{center}

Answers to attack traffic without dns-proxy running is exceptionally slow as bind9 has to search through all of its records to see if any match for each request. Answers to attack traffic with dns-proxy running is extremely fast as all packet processing is done in DPDK, and the packet does not get copied to the virtual interface nor processed by bind9. \\

DPDK-pktgen could not be run against a normal bind9 server, as it was unable to log any response packets. This is most likely due the kernel driver and bind9 not being able to respond quickly enough to the requests. \\

The performance of dns-proxy against DPDK-pktgen shows that it is capable of replying to invalid TLD queries at a rate of 9.25 Mpps without packet loss on an Intel Xeon E5-2630 and an Intel X710 NIC, and at 11.1\% of line rate with under 10\% packet loss. As the hardware used in the experiment is 6 years old at the time of writing, performance should be better in more modern configurations with faster PCIe lanes, higher clock speeds, and more efficient NICs. \\

\end{document}
